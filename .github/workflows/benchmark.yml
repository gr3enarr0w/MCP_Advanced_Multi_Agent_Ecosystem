name: MCP Performance Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'quick'
        type: choice
        options:
          - quick
          - comprehensive
          - stress
          - production

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Get cache key
        id: cache-key
        run: |
          echo "key=benchmark-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}" >> $GITHUB_OUTPUT

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            benchmarks/node_modules
          key: ${{ steps.cache-key.outputs.key }}

      - name: Install dependencies
        run: |
          cd benchmarks
          npm ci

  benchmark:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - type: 'llm'
            providers: 'ollama,perplexity'
          - type: 'search'
            providers: 'tavily,perplexity,brave,duckduckgo'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            benchmarks/node_modules
          key: ${{ needs.setup.outputs.cache-key }}

      - name: Install dependencies
        run: |
          cd benchmarks
          npm ci

      - name: Run benchmarks
        env:
          OLLAMA_BASE_URL: ${{ secrets.OLLAMA_BASE_URL || 'http://localhost:11434' }}
          OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL || 'llama2' }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
          PERPLEXITY_BASE_URL: ${{ secrets.PERPLEXITY_BASE_URL || 'http://localhost:8080' }}
          PERPLEXITY_MODEL: ${{ secrets.PERPLEXITY_MODEL || 'llama-3.1-sonar-small-128k-online' }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          BRAVE_API_KEY: ${{ secrets.BRAVE_API_KEY }}
        run: |
          cd benchmarks
          npm run build
          
          # Run benchmarks based on input or default
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'quick' }}"
          
          if [ "$BENCHMARK_TYPE" = "quick" ]; then
            npm run benchmark -- --type quick --providers ${{ matrix.providers }}
          elif [ "$BENCHMARK_TYPE" = "comprehensive" ]; then
            npm run benchmark --type comprehensive --providers ${{ matrix.providers }}
          elif [ "$BENCHMARK_TYPE" = "stress" ]; then
            npm run benchmark --type stress --providers ${{ matrix.providers }}
          elif [ "$BENCHMARK_TYPE" = "production" ]; then
            npm run benchmark --type production --providers ${{ matrix.providers }}
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ matrix.type }}-${{ matrix.providers }}
          path: reports/
          retention-days: 30

      - name: Upload HTML reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-html-${{ matrix.type }}-${{ matrix.providers }}
          path: reports/*.html
          retention-days: 30

  regression-detection:
    needs: [setup, benchmark]
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Download previous benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-llm-ollama,perplexity
          path: ./previous-results
          run-id: ${{ github.event.before }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            benchmarks/node_modules
          key: benchmark-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}

      - name: Install dependencies
        run: |
          cd benchmarks
          npm ci

      - name: Run regression detection
        run: |
          cd benchmarks
          npm run build
          
          # Run regression detection
          node dist/regression-detector.js \
            --reports "./previous-results/*.json" \
            --output "./regression-report"

      - name: Upload regression report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: regression-report
          path: regression-report/
          retention-days: 90

      - name: Comment on PR with regression results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read regression report
            let regressionReport = '';
            try {
              regressionReport = fs.readFileSync('./regression-report/regression-analysis.md', 'utf8');
            } catch (error) {
              console.log('No regression report found');
              return;
            }
            
            if (regressionReport.includes('No regressions detected')) {
              console.log('No regressions to report');
              return;
            }
            
            // Create comment
            const comment = `## ðŸš¨ Performance Regression Detected\n\n${regressionReport}`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  notify:
    needs: [setup, benchmark]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: 'failure',
              target_url: 'https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}',
              description: 'Performance benchmarks failed',
              context: 'benchmark'
            });

      - name: Notify on success
        if: success()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: 'success',
              target_url: 'https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}',
              description: 'Performance benchmarks completed successfully',
              context: 'benchmark'
            });

  performance-summary:
    needs: [setup, benchmark]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: ./all-results
          merge-multiple: true

      - name: Generate performance summary
        run: |
          cd benchmarks
          npm run build
          
          # Generate summary from all results
          node dist/performance-summary.js \
            --results "./all-results/*.json" \
            --output "./performance-summary"

      - name: Upload performance summary
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-summary
          path: performance-summary/
          retention-days: 30

      - name: Update README with latest results
        if: github.ref == 'refs/heads/main'
        run: |
          cd benchmarks
          npm run build
          
          # Update README with latest benchmark data
          node dist/update-readme.js \
            --results "./all-results/*.json" \
            --output "../README.md"